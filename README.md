# ğŸŒŸ IRIS - AI Project Summary

> Une application moderne de rÃ©sumÃ© de projet en temps rÃ©el, alimentÃ©e par l'IA locale (Ollama).

![IRIS Preview](https://img.shields.io/badge/Status-Development-blue) ![React](https://img.shields.io/badge/React-18.2-61dafb) ![TypeScript](https://img.shields.io/badge/TypeScript-5.3-3178c6) ![Docker](https://img.shields.io/badge/Docker-Ready-2496ED)

## ğŸ“‹ Description

IRIS est une application full-stack qui permet de:

- **Afficher une conversation** Ã  gauche de l'Ã©cran (texte)
- **Afficher en temps rÃ©el un rÃ©sumÃ© structurÃ©** de projet Ã  droite, rempli par une IA

L'IA analyse le transcript de la conversation et extrait automatiquement:

- ğŸ“ **Titre** du projet
- ğŸ“… **Date de dÃ©but** (format YYYY-MM-DD)
- ğŸ“… **Date de fin** (format YYYY-MM-DD)
- ğŸ’° **Budget** (en euros)

## ğŸ³ DÃ©marrage avec Docker (RecommandÃ©)

### PrÃ©requis

- Docker Desktop installÃ©
- Au moins 8 Go de RAM (pour Ollama)

### Lancement en une commande

```bash
# Cloner et lancer
cd Iris
docker-compose up -d

# TÃ©lÃ©charger le modÃ¨le Ollama (premiÃ¨re fois uniquement)
docker exec -it iris-ollama ollama pull llama3.2
```

L'application sera disponible sur:

- **Frontend**: http://localhost
- **Backend API**: http://localhost:3001
- **Ollama API**: http://localhost:11434

### ArrÃªter l'application

```bash
docker-compose down
```

## ğŸ–¥ï¸ DÃ©marrage local (DÃ©veloppement)

### PrÃ©requis

- Node.js 18+ et npm
- Ollama installÃ© localement ([ollama.ai](https://ollama.ai))

### 1. Installer Ollama et le modÃ¨le

```bash
# Installer Ollama (Windows/Mac/Linux)
# TÃ©lÃ©charger depuis https://ollama.ai

# TÃ©lÃ©charger le modÃ¨le
ollama pull llama3.2

# VÃ©rifier qu'Ollama tourne
ollama list
```

### 2. Lancer l'application

```bash
# Terminal 1 - Backend
cd backend
npm install
npm run dev

# Terminal 2 - Frontend
cd frontend
npm install
npm run dev
```

L'application sera disponible sur:

- **Frontend**: http://localhost:5173
- **Backend API**: http://localhost:3001

## ğŸ—ï¸ Architecture

```
Iris/
â”œâ”€â”€ docker-compose.yml          # Orchestration Docker
â”œâ”€â”€ frontend/                   # Application React
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ components/        # Composants UI
â”‚       â”œâ”€â”€ hooks/             # Hooks personnalisÃ©s
â”‚       â”œâ”€â”€ services/          # Appels API
â”‚       â””â”€â”€ types/             # Types TypeScript
â”‚
â”œâ”€â”€ backend/                    # API Node.js/Express
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ routes/            # Routes Express
â”‚       â”œâ”€â”€ services/          # Logique mÃ©tier (IA Ollama)
â”‚       â”œâ”€â”€ repositories/      # Couche donnÃ©es
â”‚       â””â”€â”€ models/            # Types/ModÃ¨les
â”‚
â””â”€â”€ README.md
```

## ğŸ¨ Design & UX

### ThÃ¨me visuel

- **Dark futuriste** avec dÃ©gradÃ© bleu nuit/violet
- Accents **nÃ©on** (cyan `#00f5ff`, violet `#a855f7`)
- Effets **glassmorphism** et **glow** subtils
- Police moderne **Sora** + **JetBrains Mono**

### Animations (Framer Motion)

- âœ¨ **Fade + scale** Ã  l'apparition des champs
- ğŸ’« **Highlight pulsÃ©** lors des mises Ã  jour
- ğŸ“ **Slide-in** pour les nouveaux messages
- â³ **Skeleton loading** en attente

## ğŸ“¡ API Endpoints

### `POST /api/messages`

Envoie un message et reÃ§oit l'analyse IA.

```json
// Request
{
  "conversationId": "conv_123",
  "text": "Le projet s'appelle Nova, budget de 50000â‚¬"
}

// Response
{
  "aiMessage": "Bien notÃ© ! J'ai notÃ© le nom du projet...",
  "summary": {
    "title": "Nova",
    "start_date": null,
    "end_date": null,
    "budget": 50000
  }
}
```

### `GET /api/projects/:conversationId/summary`

RÃ©cupÃ¨re le rÃ©sumÃ© actuel d'une conversation.

### `GET /api/health`

VÃ©rifie l'Ã©tat du serveur et d'Ollama.

## âš™ï¸ Configuration

### Variables d'environnement (Backend)

| Variable       | Description       | DÃ©faut                   |
| -------------- | ----------------- | ------------------------ |
| `PORT`         | Port du serveur   | `3001`                   |
| `OLLAMA_HOST`  | URL d'Ollama      | `http://localhost:11434` |
| `OLLAMA_MODEL` | ModÃ¨le Ã  utiliser | `llama3.2`               |

### ModÃ¨les Ollama recommandÃ©s

| ModÃ¨le        | Taille | Performance        |
| ------------- | ------ | ------------------ |
| `llama3.2`    | 2GB    | â­â­â­ RecommandÃ©  |
| `llama3.2:1b` | 1.3GB  | â­â­ LÃ©ger         |
| `mistral`     | 4GB    | â­â­â­â­ Meilleur  |
| `mixtral`     | 26GB   | â­â­â­â­â­ Premium |

## ğŸ”Œ IntÃ©grations futures

### Speech-to-Text (STT)

Le hook `useTranscriptStream` est prÃªt pour l'intÃ©gration:

```typescript
// frontend/src/hooks/useTranscriptStream.ts
// TODO: Brancher ici le rÃ©sultat du package STT
```

### MongoDB

Le repository est prÃªt pour MongoDB:

```typescript
// backend/src/repositories/projectSummaryRepository.ts
// TODO: Remplacer par une vraie implÃ©mentation MongoDB
```

## ğŸ§ª Test rapide

1. Lancez l'application (Docker ou local)
2. Assurez-vous qu'Ollama est dÃ©marrÃ© avec le modÃ¨le
3. Tapez dans le chat:
   - _"Le projet s'appelle Aurora"_
   - _"On commence le 15 janvier 2025"_
   - _"Le budget est de 75000 euros"_
4. Observez le panneau droit se remplir en temps rÃ©el ! âœ¨

## ğŸ› ï¸ Technologies

### Frontend

- **React 18** + TypeScript
- **Tailwind CSS** pour le styling
- **Framer Motion** pour les animations
- **Lucide React** pour les icÃ´nes
- **Vite** comme bundler

### Backend

- **Node.js** + **Express**
- **TypeScript**
- **Zod** pour la validation
- **Ollama** pour l'IA locale

### Infrastructure

- **Docker** + **Docker Compose**
- **Nginx** pour le frontend
- **Ollama** comme LLM local

## ğŸ“„ License

MIT Â© 2024 IRIS Team
